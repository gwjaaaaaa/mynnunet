"""
DSR - Dynamic Spectral Routing Module
åŠ¨æ€å…‰è°±è·¯ç”±æ¨¡å—

æ ¸å¿ƒåˆ›æ–°ï¼š
1. å¤šä¸“å®¶è·¯ç”±ç³»ç»Ÿ - ä¸åŒçš„"ä¸“å®¶"ç½‘ç»œå¤„ç†ä¸åŒçš„å…‰è°±æ¨¡å¼
2. å…‰è°±æ„ŸçŸ¥è·¯ç”±é—¨æ§ - åŸºäºå…‰è°±ç‰¹å¾åŠ¨æ€å†³å®šè·¯ç”±æƒé‡
3. è‡ªé€‚åº”ç‰¹å¾èšåˆ - è½¯è·¯ç”±æœºåˆ¶ï¼Œå…è®¸ç‰¹å¾ç»è¿‡å¤šæ¡è·¯å¾„
4. å…‰è°±-è¯­ä¹‰è”åˆå»ºæ¨¡ - åŒæ—¶è€ƒè™‘å…‰è°±ç‰¹æ€§å’Œè¯­ä¹‰ä¿¡æ¯

ç†è®ºåˆ›æ–°ç‚¹ï¼š
- å¼•å…¥ä¸“å®¶æ··åˆ(Mixture of Experts)æ€æƒ³åˆ°å…‰è°±ç‰¹å¾å¤„ç†
- å…‰è°±è·¯ç”±ç½‘ç»œå¯å­¦ä¹ ä¸åŒç»„ç»‡ç±»å‹çš„æœ€ä¼˜å¤„ç†è·¯å¾„
- åŠ¨æ€æƒé‡åˆ†é…æé«˜äº†ç½‘ç»œå¯¹å¤æ‚å…‰è°±æ¨¡å¼çš„é€‚åº”æ€§
- å¯ä»¥åˆ†æä¸åŒè·¯å¾„çš„æ¿€æ´»æ¨¡å¼ï¼Œæä¾›å¯è§£é‡Šæ€§
"""

import torch
import torch.nn as nn
import torch.nn.functional as F


class SpectralRouter(nn.Module):
    """
    å…‰è°±è·¯ç”±å™¨
    
    åˆ›æ–°ç‚¹ï¼šåŸºäºè¾“å…¥çš„å…‰è°±ç‰¹å¾åŠ¨æ€ç”Ÿæˆè·¯ç”±æƒé‡
    ä¸åŒçš„å…‰è°±æ¨¡å¼ä¼šæ¿€æ´»ä¸åŒçš„ä¸“å®¶ç½‘ç»œ
    """
    def __init__(self, channels, spectral_dim=20, num_experts=4, temperature=1.0):
        super().__init__()
        self.channels = channels
        self.spectral_dim = spectral_dim
        self.num_experts = num_experts
        self.temperature = temperature
        
        # å…‰è°±ç‰¹å¾æå–å™¨
        self.spectral_encoder = nn.Sequential(
            nn.Conv1d(channels, channels // 2, kernel_size=3, padding=1),
            nn.BatchNorm1d(channels // 2),
            nn.ReLU(inplace=True),
            nn.Conv1d(channels // 2, channels // 4, kernel_size=3, padding=1),
            nn.BatchNorm1d(channels // 4),
            nn.ReLU(inplace=True),
        )
        
        # è·¯ç”±å†³ç­–ç½‘ç»œ
        self.router = nn.Sequential(
            nn.AdaptiveAvgPool1d(1),
            nn.Flatten(),
            nn.Linear(channels // 4, num_experts * 2),
            nn.ReLU(inplace=True),
            nn.Linear(num_experts * 2, num_experts),
        )
        
        # å¯å­¦ä¹ çš„æ¸©åº¦å‚æ•°ï¼ˆç”¨äºæ§åˆ¶è·¯ç”±çš„ç¨€ç–æ€§ï¼‰
        self.temperature_param = nn.Parameter(torch.tensor(temperature))
        
    def forward(self, x):
        """
        è¾“å…¥: x (B, C, H, W, D)
        è¾“å‡º: routing_weights (B, num_experts, H, W)
        """
        B, C, H, W, D = x.shape
        
        # æå–å…¨å±€å…‰è°±ç‰¹å¾ç”¨äºè·¯ç”±å†³ç­–
        # (B, C, H, W, D) -> (B, C, D)
        global_spectral = x.mean(dim=[2, 3])  # (B, C, D)
        
        # ç¼–ç å…‰è°±ç‰¹å¾
        spectral_features = self.spectral_encoder(global_spectral)  # (B, C//4, D)
        
        # ç”Ÿæˆè·¯ç”±æƒé‡
        routing_logits = self.router(spectral_features)  # (B, num_experts)
        
        # Softmax with temperature (æ¸©åº¦è¶Šé«˜ï¼Œåˆ†å¸ƒè¶Šå¹³æ»‘)
        routing_weights = F.softmax(routing_logits / self.temperature_param, dim=1)  # (B, num_experts)
        
        # æ‰©å±•åˆ°ç©ºé—´ç»´åº¦: (B, num_experts) -> (B, num_experts, H, W)
        routing_weights = routing_weights.view(B, self.num_experts, 1, 1).expand(B, self.num_experts, H, W)
        
        return routing_weights


class SpectralExpert(nn.Module):
    """
    å…‰è°±ä¸“å®¶ç½‘ç»œ
    
    åˆ›æ–°ç‚¹ï¼šæ¯ä¸ªä¸“å®¶ä¸“æ³¨äºå¤„ç†ç‰¹å®šç±»å‹çš„å…‰è°±æ¨¡å¼
    ä½¿ç”¨ä¸åŒçš„å·ç§¯æ ¸é…ç½®æ¥é€‚åº”ä¸åŒçš„å…‰è°±ç‰¹æ€§
    """
    def __init__(self, channels, spectral_dim=20, expert_type='standard'):
        super().__init__()
        self.channels = channels
        self.spectral_dim = spectral_dim
        self.expert_type = expert_type
        
        # æ ¹æ®ä¸“å®¶ç±»å‹ä½¿ç”¨ä¸åŒçš„ç½‘ç»œç»“æ„
        if expert_type == 'spectral_focused':
            # ä¸“æ³¨äºå…‰è°±ç»´åº¦çš„ä¸“å®¶
            self.expert_net = nn.Sequential(
                nn.Conv3d(channels, channels, kernel_size=(1, 1, 5), padding=(0, 0, 2)),
                nn.BatchNorm3d(channels),
                nn.ReLU(inplace=True),
                nn.Conv3d(channels, channels, kernel_size=(1, 1, 3), padding=(0, 0, 1)),
                nn.BatchNorm3d(channels),
            )
        elif expert_type == 'spatial_focused':
            # ä¸“æ³¨äºç©ºé—´ç»´åº¦çš„ä¸“å®¶
            self.expert_net = nn.Sequential(
                nn.Conv3d(channels, channels, kernel_size=(5, 5, 1), padding=(2, 2, 0)),
                nn.BatchNorm3d(channels),
                nn.ReLU(inplace=True),
                nn.Conv3d(channels, channels, kernel_size=(3, 3, 1), padding=(1, 1, 0)),
                nn.BatchNorm3d(channels),
            )
        elif expert_type == 'fine_grained':
            # ç»†ç²’åº¦ç‰¹å¾ä¸“å®¶
            self.expert_net = nn.Sequential(
                nn.Conv3d(channels, channels * 2, kernel_size=1),
                nn.BatchNorm3d(channels * 2),
                nn.ReLU(inplace=True),
                nn.Conv3d(channels * 2, channels, kernel_size=1),
                nn.BatchNorm3d(channels),
            )
        else:  # 'standard'
            # æ ‡å‡†ä¸“å®¶
            self.expert_net = nn.Sequential(
                nn.Conv3d(channels, channels, kernel_size=3, padding=1),
                nn.BatchNorm3d(channels),
                nn.ReLU(inplace=True),
                nn.Conv3d(channels, channels, kernel_size=3, padding=1),
                nn.BatchNorm3d(channels),
            )
    
    def forward(self, x):
        """
        è¾“å…¥: x (B, C, H, W, D)
        è¾“å‡º: processed_x (B, C, H, W, D)
        """
        return self.expert_net(x)


class AdaptiveFeatureAggregator(nn.Module):
    """
    è‡ªé€‚åº”ç‰¹å¾èšåˆå™¨
    
    åˆ›æ–°ç‚¹ï¼šä¸æ˜¯ç®€å•çš„åŠ æƒæ±‚å’Œï¼Œè€Œæ˜¯å­¦ä¹ å¦‚ä½•æœ€ä¼˜åœ°èåˆå¤šä¸ªä¸“å®¶çš„è¾“å‡º
    """
    def __init__(self, channels, num_experts=4):
        super().__init__()
        self.channels = channels
        self.num_experts = num_experts
        
        # ç‰¹å¾èåˆç½‘ç»œ
        self.fusion_conv = nn.Sequential(
            nn.Conv3d(channels * num_experts, channels * 2, kernel_size=1),
            nn.BatchNorm3d(channels * 2),
            nn.ReLU(inplace=True),
            nn.Conv3d(channels * 2, channels, kernel_size=1),
            nn.BatchNorm3d(channels),
        )
        
        # é—¨æ§ç½‘ç»œï¼šå†³å®šå¦‚ä½•èåˆä¸åŒä¸“å®¶
        self.gating = nn.Sequential(
            nn.AdaptiveAvgPool3d(1),
            nn.Conv3d(channels, channels, kernel_size=1),
            nn.Sigmoid()
        )
        
    def forward(self, expert_outputs, routing_weights):
        """
        è¾“å…¥:
            - expert_outputs: list of (B, C, H, W, D), length=num_experts
            - routing_weights: (B, num_experts, H, W)
        è¾“å‡º:
            - aggregated: (B, C, H, W, D)
        """
        B, C, H, W, D = expert_outputs[0].shape
        
        # æ–¹æ³•1: åŠ æƒæ±‚å’Œï¼ˆåŸºç¡€èšåˆï¼‰
        weighted_sum = torch.zeros_like(expert_outputs[0])
        for i, expert_out in enumerate(expert_outputs):
            # routing_weights[:, i, :, :] -> (B, H, W)
            # æ‰©å±•ç»´åº¦: (B, 1, H, W, 1)
            weight = routing_weights[:, i, :, :].unsqueeze(1).unsqueeze(-1)  # (B, 1, H, W, 1)
            weighted_sum = weighted_sum + weight * expert_out
        
        # æ–¹æ³•2: å­¦ä¹ çš„èåˆï¼ˆé«˜çº§èšåˆï¼‰
        # æ‹¼æ¥æ‰€æœ‰ä¸“å®¶è¾“å‡º
        concat_features = torch.cat(expert_outputs, dim=1)  # (B, C*num_experts, H, W, D)
        
        # é€šè¿‡å·ç§¯èåˆ
        fused_features = self.fusion_conv(concat_features)  # (B, C, H, W, D)
        
        # é—¨æ§æœºåˆ¶ï¼šå†³å®šä½¿ç”¨åŠ æƒæ±‚å’Œè¿˜æ˜¯å­¦ä¹ èåˆ
        gate = self.gating(fused_features)  # (B, C, 1, 1, 1)
        
        # è‡ªé€‚åº”ç»„åˆ
        output = gate * fused_features + (1 - gate) * weighted_sum
        
        return output


class DSRModule(nn.Module):
    """
    å®Œæ•´çš„DSRæ¨¡å—
    
    åˆ›æ–°ç‚¹æ€»ç»“ï¼š
    1. å¤šä¸“å®¶ç³»ç»Ÿ - 4ä¸ªä¸“å®¶ç½‘ç»œï¼Œå„æœ‰ä¸“é•¿
    2. åŠ¨æ€è·¯ç”± - åŸºäºå…‰è°±ç‰¹å¾è‡ªåŠ¨é€‰æ‹©å¤„ç†è·¯å¾„
    3. è½¯è·¯ç”±æœºåˆ¶ - å…è®¸ç‰¹å¾ç»è¿‡å¤šæ¡è·¯å¾„ï¼ˆä¸æ˜¯ç¡¬è·¯ç”±ï¼‰
    4. è‡ªé€‚åº”èšåˆ - æ™ºèƒ½èåˆå¤šä¸ªä¸“å®¶çš„è¾“å‡º
    
    è¾“å…¥: (B, C, H, W, D)
    è¾“å‡º: (B, C, H, W, D)  # å°ºåº¦ä¿æŒä¸å˜
    """
    def __init__(self, 
                 channels, 
                 spectral_dim=20, 
                 num_experts=4,
                 expert_types=None,
                 temperature=1.0,
                 use_residual=True):
        super().__init__()
        
        self.channels = channels
        self.spectral_dim = spectral_dim
        self.num_experts = num_experts
        self.use_residual = use_residual
        
        # é»˜è®¤çš„ä¸“å®¶ç±»å‹é…ç½®
        if expert_types is None:
            expert_types = ['spectral_focused', 'spatial_focused', 'fine_grained', 'standard']
        assert len(expert_types) == num_experts, "expert_typesæ•°é‡å¿…é¡»ç­‰äºnum_experts"
        
        # 1. å…‰è°±è·¯ç”±å™¨
        self.router = SpectralRouter(channels, spectral_dim, num_experts, temperature)
        
        # 2. å¤šä¸ªä¸“å®¶ç½‘ç»œ
        self.experts = nn.ModuleList([
            SpectralExpert(channels, spectral_dim, expert_type=expert_types[i])
            for i in range(num_experts)
        ])
        
        # 3. è‡ªé€‚åº”ç‰¹å¾èšåˆå™¨
        self.aggregator = AdaptiveFeatureAggregator(channels, num_experts)
        
        # 4. è¾“å‡ºå¢å¼º
        self.output_enhance = nn.Sequential(
            nn.Conv3d(channels, channels, kernel_size=1),
            nn.BatchNorm3d(channels),
        )
        
        print(f"  [DSR] Initialized:")
        print(f"    - Channels: {channels}")
        print(f"    - Num experts: {num_experts}")
        print(f"    - Expert types: {expert_types}")
        print(f"    - Temperature: {temperature}")
    
    def forward(self, x):
        """
        è¾“å…¥: x (B, C, H, W, D)
        è¾“å‡º: routed_x (B, C, H, W, D)
        
        å¤„ç†æµç¨‹ï¼š
        1. è·¯ç”±å™¨å†³å®šæ¯ä¸ªä¸“å®¶çš„æƒé‡
        2. æ‰€æœ‰ä¸“å®¶å¹¶è¡Œå¤„ç†è¾“å…¥
        3. æ ¹æ®è·¯ç”±æƒé‡èšåˆä¸“å®¶è¾“å‡º
        4. æ®‹å·®è¿æ¥
        """
        identity = x  # æ®‹å·®è¿æ¥
        
        # Step 1: ç”Ÿæˆè·¯ç”±æƒé‡
        routing_weights = self.router(x)  # (B, num_experts, H, W)
        
        # Step 2: æ‰€æœ‰ä¸“å®¶å¹¶è¡Œå¤„ç†
        expert_outputs = []
        for expert in self.experts:
            expert_out = expert(x)  # (B, C, H, W, D)
            expert_outputs.append(expert_out)
        
        # Step 3: è‡ªé€‚åº”èšåˆ
        aggregated = self.aggregator(expert_outputs, routing_weights)  # (B, C, H, W, D)
        
        # Step 4: è¾“å‡ºå¢å¼º
        output = self.output_enhance(aggregated)
        
        # Step 5: æ®‹å·®è¿æ¥
        if self.use_residual:
            output = output + identity
        
        return output
    
    def get_routing_statistics(self, x):
        """
        è·å–è·¯ç”±ç»Ÿè®¡ä¿¡æ¯ï¼Œç”¨äºåˆ†æå’Œå¯è§†åŒ–
        è¿”å›æ¯ä¸ªä¸“å®¶çš„å¹³å‡æ¿€æ´»æƒé‡
        """
        with torch.no_grad():
            routing_weights = self.router(x)  # (B, num_experts, H, W)
            avg_weights = routing_weights.mean(dim=[0, 2, 3])  # (num_experts,)
        return avg_weights.cpu().numpy()


class DSRModuleLight(nn.Module):
    """
    DSRè½»é‡ç‰ˆ - ç”¨äºæ—©æœŸstage
    å‡å°‘ä¸“å®¶æ•°é‡å’Œå¤æ‚åº¦
    """
    def __init__(self, 
                 channels, 
                 spectral_dim=20, 
                 num_experts=2):
        super().__init__()
        
        self.channels = channels
        self.num_experts = num_experts
        
        # ç®€åŒ–çš„è·¯ç”±å™¨
        self.router = nn.Sequential(
            nn.AdaptiveAvgPool3d((1, 1, spectral_dim)),
            nn.Conv3d(channels, num_experts, kernel_size=1),
            nn.Softmax(dim=1)
        )
        
        # ä¸¤ä¸ªç®€å•çš„ä¸“å®¶
        self.expert1 = nn.Conv3d(channels, channels, kernel_size=3, padding=1)
        self.expert2 = nn.Conv3d(channels, channels, kernel_size=3, padding=2, dilation=2)
        
        print(f"  [DSR-Light] Channels={channels}, Experts={num_experts}")
    
    def forward(self, x):
        identity = x
        
        # è·¯ç”±æƒé‡
        weights = self.router(x)  # (B, num_experts, 1, 1, D)
        w1 = weights[:, 0:1, :, :, :]  # (B, 1, 1, 1, D)
        w2 = weights[:, 1:2, :, :, :]
        
        # ä¸“å®¶å¤„ç†
        out1 = self.expert1(x)
        out2 = self.expert2(x)
        
        # åŠ æƒæ±‚å’Œ
        output = w1 * out1 + w2 * out2
        
        return output + identity


class DSRModuleEfficientLite(nn.Module):
    """
    ğŸš€ é«˜æ•ˆè½»é‡çº§DSR - è®ºæ–‡å‹å¥½ç‰ˆæœ¬
    
    ä¼˜åŒ–ç­–ç•¥ï¼š
    1. âœ“ å‡å°‘experts: 4 â†’ 2ï¼ˆè®¡ç®—é‡å‡åŠï¼‰
    2. âœ“ Depthwise separable expertsï¼ˆå‚æ•°-70%ï¼‰
    3. âœ“ è½»é‡routing networkï¼ˆhidden_dim = channels//8ï¼‰
    4. âœ“ å…¨å±€æ± åŒ–routingï¼ˆæ— spatial overheadï¼‰
    5. âœ“ åªåœ¨encoderå…³é”®å±‚ä½¿ç”¨
    
    ä¿ç•™åˆ›æ–°ç‚¹ï¼š
    âœ… å¤šexpertæ··åˆç³»ç»Ÿ
    âœ… å…‰è°±æ„ŸçŸ¥åŠ¨æ€è·¯ç”±
    âœ… è‡ªé€‚åº”ç‰¹å¾èšåˆ
    âœ… å¯è§£é‡Šè·¯ç”±æƒé‡
    
    æ˜¾å­˜å‡å°‘: ~50%
    é€Ÿåº¦æå‡: ~45%
    """
    def __init__(self, channels, spectral_dim=60, num_experts=2, lightweight_experts=True):
        super().__init__()
        self.channels = channels
        self.num_experts = num_experts
        self.lightweight_experts = lightweight_experts
        
        # 1. è¶…è½»é‡routing networkï¼ˆå‚æ•°é‡æå°ï¼‰
        self.routing = nn.Sequential(
            nn.AdaptiveAvgPool3d((1, 1, 1)),  # å…¨å±€æ± åŒ–
            nn.Flatten(),
            nn.Linear(channels, max(channels // 8, num_experts * 2)),
            nn.ReLU(inplace=True),
            nn.Linear(max(channels // 8, num_experts * 2), num_experts),
        )
        
        # Temperature for softmax (learnable)
        self.temperature = nn.Parameter(torch.tensor(1.0))
        
        # 2. Depthwise Separable Expertsï¼ˆæè‡´è½»é‡ï¼‰
        if lightweight_experts:
            # Expert 1: å…‰è°±ä¸“å®¶ (focus on spectral dimension)
            self.expert1 = nn.Sequential(
                # Depthwise: åˆ†ç¦»å¤„ç†æ¯ä¸ªchannel
                nn.Conv3d(channels, channels, kernel_size=(1, 1, 3), 
                         padding=(0, 0, 1), groups=channels),
                nn.BatchNorm3d(channels),
                nn.ReLU(inplace=True),
                # Pointwise: è·¨channelèåˆ
                nn.Conv3d(channels, channels, kernel_size=1),
                nn.BatchNorm3d(channels),
            )
            
            # Expert 2: ç©ºé—´ä¸“å®¶ (focus on spatial dimension)
            self.expert2 = nn.Sequential(
                # Depthwise
                nn.Conv3d(channels, channels, kernel_size=(3, 3, 1), 
                         padding=(1, 1, 0), groups=channels),
                nn.BatchNorm3d(channels),
                nn.ReLU(inplace=True),
                # Pointwise
                nn.Conv3d(channels, channels, kernel_size=1),
                nn.BatchNorm3d(channels),
            )
        else:
            # æ ‡å‡†experts (æ›´é‡)
            self.expert1 = nn.Sequential(
                nn.Conv3d(channels, channels, kernel_size=(1, 1, 3), padding=(0, 0, 1)),
                nn.BatchNorm3d(channels),
                nn.ReLU(inplace=True),
            )
            self.expert2 = nn.Sequential(
                nn.Conv3d(channels, channels, kernel_size=(3, 3, 1), padding=(1, 1, 0)),
                nn.BatchNorm3d(channels),
                nn.ReLU(inplace=True),
            )
        
        # 3. è½»é‡fusion (1x1 conv)
        self.fusion = nn.Conv3d(channels, channels, kernel_size=1)
        
        # è®¡ç®—å‚æ•°é‡
        routing_params = sum(p.numel() for p in self.routing.parameters())
        expert_params = (sum(p.numel() for p in self.expert1.parameters()) + 
                        sum(p.numel() for p in self.expert2.parameters()))
        total_params = routing_params + expert_params
        
        print(f"  [DSR-Efficient-Lite] C={channels}, Experts={num_experts}")
        print(f"    â”œâ”€ Routing params: {routing_params:,}")
        print(f"    â”œâ”€ Expert params: {expert_params:,}")
        print(f"    â”œâ”€ Total: {total_params:,} (vs ~{total_params*4:,.0f} in full DSR)")
        print(f"    â””â”€ Lightweight experts: {lightweight_experts}")
    
    def forward(self, x):
        """
        è¾“å…¥: x (B, C, H, W, D)
        è¾“å‡º: routed (B, C, H, W, D)
        """
        B, C, H, W, D = x.shape
        identity = x
        
        # Step 1: è®¡ç®—routing weightsï¼ˆåœ¨å…¨å±€æ± åŒ–åçš„ç‰¹å¾ä¸Šï¼‰
        routing_logits = self.routing(x)  # (B, num_experts)
        routing_weights = F.softmax(routing_logits / self.temperature, dim=1)  # (B, 2)
        
        # Step 2: Expert processingï¼ˆå¹¶è¡Œï¼‰
        expert_outputs = []
        expert_outputs.append(self.expert1(x))  # (B, C, H, W, D)
        expert_outputs.append(self.expert2(x))  # (B, C, H, W, D)
        
        # Step 3: åŠ æƒèšåˆ
        # routing_weights: (B, 2) -> (B, 2, 1, 1, 1, 1)
        weights_expanded = routing_weights.view(B, self.num_experts, 1, 1, 1, 1)
        
        # Stack experts: (num_experts, B, C, H, W, D) -> (B, num_experts, C, H, W, D)
        stacked_experts = torch.stack(expert_outputs, dim=1)  # (B, 2, C, H, W, D)
        
        # Weighted sum: (B, 2, C, H, W, D) * (B, 2, 1, 1, 1, 1) -> (B, 2, C, H, W, D) -> (B, C, H, W, D)
        weighted = stacked_experts * weights_expanded
        aggregated = weighted.sum(dim=1)  # (B, C, H, W, D)
        
        # Step 4: è½»é‡èåˆ + æ®‹å·®
        output = self.fusion(aggregated)
        output = output + identity
        
        return output
    
    def get_routing_weights(self, x):
        """è·å–routing weightsç”¨äºå¯è§†åŒ–å’Œåˆ†æï¼ˆä¿ç•™å¯è§£é‡Šæ€§ï¼‰"""
        with torch.no_grad():
            routing_logits = self.routing(x)
            routing_weights = F.softmax(routing_logits / self.temperature, dim=1)
        return routing_weights.cpu()


class DSRModuleEfficientLite4Experts(nn.Module):
    """
    ğŸš€ğŸš€ é«˜æ•ˆè½»é‡çº§DSR - 4ä¸“å®¶å¢å¼ºç‰ˆ
    
    æ–°å¢åŠŸèƒ½ï¼š
    1. âœ… æ”¯æŒ4ä¸ªä¸“å®¶ï¼ˆå…‰è°±ã€ç©ºé—´ã€ç»†ç²’åº¦ã€æ ‡å‡†ï¼‰
    2. âœ… æ‰€æœ‰ä¸“å®¶ä½¿ç”¨Depthwise Separableä¿æŒè½»é‡
    3. âœ… åŠ¨æ€è·¯ç”±åˆ°4ä¸ªä¸“å®¶
    4. âœ… æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›
    
    ä¸“å®¶é…ç½®ï¼š
    - Expert 1: å…‰è°±ä¸“å®¶ (1Ã—1Ã—3 kernel)
    - Expert 2: ç©ºé—´ä¸“å®¶ (3Ã—3Ã—1 kernel)
    - Expert 3: ç»†ç²’åº¦ä¸“å®¶ (1Ã—1Ã—1 pointwise, channel expansion)
    - Expert 4: æ ‡å‡†ä¸“å®¶ (3Ã—3Ã—3 kernel)
    
    å‚æ•°é‡: çº¦ä¸º2ä¸“å®¶ç‰ˆæœ¬çš„ 2å€ï¼Œä½†è¿œå°äºå®Œæ•´DSRModule
    """
    def __init__(self, channels, spectral_dim=60, lightweight_experts=True):
        super().__init__()
        self.channels = channels
        self.num_experts = 4
        self.lightweight_experts = lightweight_experts
        
        # 1. è½»é‡routing networkï¼ˆè¾“å‡º4ä¸ªä¸“å®¶çš„æƒé‡ï¼‰
        self.routing = nn.Sequential(
            nn.AdaptiveAvgPool3d((1, 1, 1)),
            nn.Flatten(),
            nn.Linear(channels, max(channels // 8, 8)),  # è‡³å°‘8ç»´hidden
            nn.ReLU(inplace=True),
            nn.Linear(max(channels // 8, 8), 4),  # è¾“å‡º4ä¸ªlogits
        )
        
        # Temperature for softmax
        self.temperature = nn.Parameter(torch.tensor(1.0))
        
        # 2. å››ä¸ªä¸“å®¶ç½‘ç»œ
        if lightweight_experts:
            # Expert 1: å…‰è°±ä¸“å®¶ (Depthwise Separable)
            self.expert1 = nn.Sequential(
                nn.Conv3d(channels, channels, kernel_size=(1, 1, 3), 
                         padding=(0, 0, 1), groups=channels),
                nn.BatchNorm3d(channels),
                nn.ReLU(inplace=True),
                nn.Conv3d(channels, channels, kernel_size=1),
                nn.BatchNorm3d(channels),
            )
            
            # Expert 2: ç©ºé—´ä¸“å®¶ (Depthwise Separable)
            self.expert2 = nn.Sequential(
                nn.Conv3d(channels, channels, kernel_size=(3, 3, 1), 
                         padding=(1, 1, 0), groups=channels),
                nn.BatchNorm3d(channels),
                nn.ReLU(inplace=True),
                nn.Conv3d(channels, channels, kernel_size=1),
                nn.BatchNorm3d(channels),
            )
            
            # Expert 3: ç»†ç²’åº¦ä¸“å®¶ (Pointwise expansion-compression)
            self.expert3 = nn.Sequential(
                nn.Conv3d(channels, channels * 2, kernel_size=1),
                nn.BatchNorm3d(channels * 2),
                nn.ReLU(inplace=True),
                nn.Conv3d(channels * 2, channels, kernel_size=1),
                nn.BatchNorm3d(channels),
            )
            
            # Expert 4: æ ‡å‡†3Dä¸“å®¶ (Depthwise Separable 3D)
            self.expert4 = nn.Sequential(
                nn.Conv3d(channels, channels, kernel_size=3, 
                         padding=1, groups=channels),
                nn.BatchNorm3d(channels),
                nn.ReLU(inplace=True),
                nn.Conv3d(channels, channels, kernel_size=1),
                nn.BatchNorm3d(channels),
            )
        else:
            # æ ‡å‡†å·ç§¯ç‰ˆæœ¬ï¼ˆå‚æ•°æ›´å¤šï¼‰
            self.expert1 = nn.Sequential(
                nn.Conv3d(channels, channels, kernel_size=(1, 1, 3), padding=(0, 0, 1)),
                nn.BatchNorm3d(channels),
                nn.ReLU(inplace=True),
            )
            self.expert2 = nn.Sequential(
                nn.Conv3d(channels, channels, kernel_size=(3, 3, 1), padding=(1, 1, 0)),
                nn.BatchNorm3d(channels),
                nn.ReLU(inplace=True),
            )
            self.expert3 = nn.Sequential(
                nn.Conv3d(channels, channels * 2, kernel_size=1),
                nn.BatchNorm3d(channels * 2),
                nn.ReLU(inplace=True),
                nn.Conv3d(channels * 2, channels, kernel_size=1),
                nn.BatchNorm3d(channels),
            )
            self.expert4 = nn.Sequential(
                nn.Conv3d(channels, channels, kernel_size=3, padding=1),
                nn.BatchNorm3d(channels),
                nn.ReLU(inplace=True),
            )
        
        # 3. è½»é‡fusion
        self.fusion = nn.Conv3d(channels, channels, kernel_size=1)
        
        # ç»Ÿè®¡å‚æ•°é‡
        routing_params = sum(p.numel() for p in self.routing.parameters())
        expert_params = (sum(p.numel() for p in self.expert1.parameters()) + 
                        sum(p.numel() for p in self.expert2.parameters()) +
                        sum(p.numel() for p in self.expert3.parameters()) +
                        sum(p.numel() for p in self.expert4.parameters()))
        fusion_params = sum(p.numel() for p in self.fusion.parameters())
        total_params = routing_params + expert_params + fusion_params
        
        print(f"  [DSR-Efficient-Lite-4Experts] C={channels}, Experts=4")
        print(f"    â”œâ”€ Routing params: {routing_params:,}")
        print(f"    â”œâ”€ Expert params: {expert_params:,}")
        print(f"    â”œâ”€ Fusion params: {fusion_params:,}")
        print(f"    â”œâ”€ Total: {total_params:,}")
        print(f"    â”œâ”€ Lightweight experts: {lightweight_experts}")
        print(f"    â””â”€ Expert types: [Spectral, Spatial, Fine-grained, Standard-3D]")
    
    def forward(self, x):
        """
        è¾“å…¥: x (B, C, H, W, D)
        è¾“å‡º: routed (B, C, H, W, D)
        """
        B, C, H, W, D = x.shape
        identity = x
        
        # Step 1: è®¡ç®—4ä¸ªä¸“å®¶çš„routing weights
        routing_logits = self.routing(x)  # (B, 4)
        routing_weights = F.softmax(routing_logits / self.temperature, dim=1)  # (B, 4)
        
        # Step 2: 4ä¸ªä¸“å®¶å¹¶è¡Œå¤„ç†
        expert_outputs = []
        expert_outputs.append(self.expert1(x))  # å…‰è°±
        expert_outputs.append(self.expert2(x))  # ç©ºé—´
        expert_outputs.append(self.expert3(x))  # ç»†ç²’åº¦
        expert_outputs.append(self.expert4(x))  # æ ‡å‡†3D
        
        # Step 3: åŠ æƒèšåˆ
        weights_expanded = routing_weights.view(B, 4, 1, 1, 1, 1)
        stacked_experts = torch.stack(expert_outputs, dim=1)  # (B, 4, C, H, W, D)
        weighted = stacked_experts * weights_expanded
        aggregated = weighted.sum(dim=1)  # (B, C, H, W, D)
        
        # Step 4: èåˆ + æ®‹å·®
        output = self.fusion(aggregated)
        output = output + identity
        
        return output
    
    def get_routing_weights(self, x):
        """è·å–4ä¸ªä¸“å®¶çš„routing weights"""
        with torch.no_grad():
            routing_logits = self.routing(x)
            routing_weights = F.softmax(routing_logits / self.temperature, dim=1)
        return routing_weights.cpu()
    
    def get_routing_statistics(self, x):
        """è¿”å›ä¸“å®¶åç§°å’Œæƒé‡ï¼ˆç”¨äºåˆ†æï¼‰"""
        weights = self.get_routing_weights(x).mean(dim=0).numpy()
        expert_names = ['Spectral', 'Spatial', 'Fine-grained', 'Standard-3D']
        return dict(zip(expert_names, weights))


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("="*70)
    print("Testing DSR Module")
    print("="*70)
    
    # æµ‹è¯•å‚æ•°
    B, C, H, W, D = 2, 64, 32, 32, 20
    
    # åˆ›å»ºæ¨¡å—
    print("\n1. Creating DSR module...")
    dsr = DSRModule(channels=C, spectral_dim=D, num_experts=4)
    
    # åˆ›å»ºè¾“å…¥
    print(f"\n2. Input shape: (B={B}, C={C}, H={H}, W={W}, D={D})")
    x = torch.randn(B, C, H, W, D)
    
    # å‰å‘ä¼ æ’­
    print("\n3. Forward pass...")
    with torch.no_grad():
        out = dsr(x)
    
    print(f"   Output shape: {out.shape}")
    assert out.shape == x.shape, "Shape mismatch!"
    
    # è·å–è·¯ç”±ç»Ÿè®¡
    print("\n4. Routing statistics:")
    routing_stats = dsr.get_routing_statistics(x)
    for i, weight in enumerate(routing_stats):
        print(f"   Expert {i}: {weight:.4f}")
    print(f"   Sum: {routing_stats.sum():.4f} (should be ~1.0)")
    
    # æµ‹è¯•ä¸åŒå°ºåº¦
    print("\n5. Testing different scales...")
    test_configs = [
        (32, 64, 64),   # æ—©æœŸstage
        (128, 32, 32),  # ä¸­æœŸ
        (320, 16, 16),  # åæœŸ
    ]
    
    for C_test, H_test, W_test in test_configs:
        x_test = torch.randn(2, C_test, H_test, W_test, 20)
        dsr_test = DSRModule(C_test, 20, num_experts=4)
        with torch.no_grad():
            out_test = dsr_test(x_test)
        assert out_test.shape == x_test.shape
        print(f"   âœ“ C={C_test}, H={H_test}, W={W_test}: OK")
    
    # æµ‹è¯•è½»é‡ç‰ˆ
    print("\n6. Testing DSR-Light...")
    dsr_light = DSRModuleLight(64, 20, num_experts=2)
    with torch.no_grad():
        out_light = dsr_light(x)
    assert out_light.shape == x.shape
    print(f"   âœ“ DSR-Light output shape: {out_light.shape}")
    
    # å‚æ•°é‡ç»Ÿè®¡
    print("\n7. Parameter count:")
    total_params = sum(p.numel() for p in dsr.parameters())
    trainable_params = sum(p.numel() for p in dsr.parameters() if p.requires_grad)
    print(f"   DSR Total: {total_params:,}")
    print(f"   DSR Trainable: {trainable_params:,}")
    
    total_params_light = sum(p.numel() for p in dsr_light.parameters())
    print(f"   DSR-Light Total: {total_params_light:,}")
    
    print("\n" + "="*70)
    print("âœ… All tests passed!")
    print("="*70)












